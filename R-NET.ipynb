{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.100d.txt'\n",
    "\n",
    "def loadEmbeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadEmbeddings(filename)\n",
    "\n",
    "\n",
    "word_vec_dim = len(embd[0])\n",
    "\n",
    "vocab.append('<UNK>')\n",
    "embd.append(np.asarray(embd[vocab.index('unk')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<EOS>')\n",
    "embd.append(np.asarray(embd[vocab.index('eos')],np.float32)+0.01)\n",
    "\n",
    "vocab.append('<PAD>')\n",
    "embd.append(np.zeros((word_vec_dim),np.float32))\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('embeddingPICKLE', 'rb') as fp:\n",
    "    processed_data = pickle.load(fp)\n",
    "\n",
    "fact_stories = processed_data[0]\n",
    "questions = processed_data[1]\n",
    "answers = np.reshape(processed_data[2],(len(processed_data[2])))\n",
    "test_fact_stories = processed_data[3]\n",
    "test_questions = processed_data[4]\n",
    "test_answers = np.reshape(processed_data[5],(len(processed_data[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE DATA:\n",
      "\n",
      "FACTS:\n",
      "\n",
      "1)  ['lily', 'is', 'a', 'frog']\n",
      "2)  ['brian', 'is', 'a', 'rhino']\n",
      "3)  ['bernhard', 'is', 'a', 'frog']\n",
      "4)  ['bernhard', 'is', 'green', '<PAD>']\n",
      "5)  ['greg', 'is', 'a', 'lion']\n",
      "6)  ['julius', 'is', 'a', 'lion']\n",
      "7)  ['brian', 'is', 'green', '<PAD>']\n",
      "8)  ['julius', 'is', 'gray', '<PAD>']\n",
      "9)  ['lily', 'is', 'green', '<PAD>']\n",
      "\n",
      "QUESTION:\n",
      "['what', 'color', 'is', 'greg']\n",
      "\n",
      "ANSWER:\n",
      "gray\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print \"EXAMPLE DATA:\\n\"\n",
    "\n",
    "sample = random.randint(0,len(fact_stories))\n",
    "\n",
    "print \"FACTS:\\n\"\n",
    "for i in xrange(len(fact_stories[sample])):\n",
    "    print str(i+1)+\") \",\n",
    "    print map(vec2word,fact_stories[sample][i])\n",
    "    \n",
    "print \"\\nQUESTION:\"\n",
    "print map(vec2word,questions[sample])\n",
    "print \"\\nANSWER:\"\n",
    "print vocab[answers[sample]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "train_fact_stories = []\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "val_fact_stories = []\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "\n",
    "p=90 #(90% data used for training. Rest for validation)\n",
    "    \n",
    "train_len = int((p/100)*len(fact_stories))\n",
    "val_len = int(((100-p)/100)*len(fact_stories))\n",
    "\n",
    "train_fact_stories = fact_stories[0:train_len] \n",
    "val_fact_stories = fact_stories[train_len:(train_len+val_len)]\n",
    "\n",
    "train_questions = questions[0:train_len] \n",
    "val_questions = questions[train_len:(train_len+val_len)] \n",
    "\n",
    "train_answers = answers[0:train_len] \n",
    "val_answers = answers[train_len:(train_len+val_len)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "\n",
    "def sentence_reader(fact_stories): #positional_encoder\n",
    "    \n",
    "    PAD_val = np.zeros((word_vec_dim),np.float32)\n",
    "    \n",
    "    pe_fact_stories = np.zeros((fact_stories.shape[0],fact_stories.shape[1],word_vec_dim),np.float32)\n",
    "    \n",
    "    for fact_story_index in xrange(0,len(fact_stories)):\n",
    "        for fact_index in xrange(0,len(fact_stories[fact_story_index])):\n",
    "            \n",
    "            M = 0\n",
    "            \n",
    "            # Code to ignore pads. \n",
    "            for word_position in xrange(len(fact_stories[fact_story_index,fact_index])):\n",
    "                if np.all(np.equal(PAD_val,fact_stories[fact_story_index,fact_index,word_position])):\n",
    "                    break\n",
    "                else:\n",
    "                    M+=1\n",
    "                \n",
    "            l = np.zeros((word_vec_dim),np.float32) \n",
    "            \n",
    "            # ljd = (1 − j/M) − (d/D)(1 − 2j/M),\n",
    "            \n",
    "            for word_position in xrange(0,M):\n",
    "                \n",
    "                for dimension in xrange(0,word_vec_dim):\n",
    "                    \n",
    "                    j = word_position + 1 # making position start from 1 instead of 0\n",
    "                    d = dimension + 1 # making dimensions start from 1 isntead of 0 (1-100 instead of 0-99)\n",
    "                    \n",
    "                    l[dimension] = (1-(j/M)) - (d/word_vec_dim)*(1-2*(j/M))\n",
    " \n",
    "                \n",
    "                pe_fact_stories[fact_story_index,fact_index] += np.multiply(l,fact_stories[fact_story_index,fact_index,word_position])\n",
    "\n",
    "\n",
    "    return pe_fact_stories\n",
    "\n",
    "train_fact_stories = sentence_reader(train_fact_stories)\n",
    "val_fact_stories = sentence_reader(val_fact_stories)\n",
    "test_fact_stories = sentence_reader(test_fact_stories)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(fact_stories,questions,answers,batch_size):\n",
    "    \n",
    "    shuffle = np.arange(len(questions))\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    batches_fact_stories = []\n",
    "    batches_questions = []\n",
    "    batches_answers = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    while i+batch_size<=len(questions):\n",
    "        batch_fact_stories = []\n",
    "        batch_questions = []\n",
    "        batch_answers = []\n",
    "        \n",
    "        for j in xrange(i,i+batch_size):\n",
    "            batch_fact_stories.append(fact_stories[shuffle[j]])\n",
    "            batch_questions.append(questions[shuffle[j]])\n",
    "            batch_answers.append(answers[shuffle[j]])\n",
    "\n",
    "        batches_fact_stories.append(batch_fact_stories)\n",
    "        batches_questions.append(batch_questions)\n",
    "        batches_answers.append(batch_answers)\n",
    "        \n",
    "        i+=batch_size\n",
    "        \n",
    "    batches_fact_stories = np.asarray(batches_fact_stories,np.float32)\n",
    "    batches_questions = np.asarray(batches_questions,np.float32)\n",
    "    batches_answers = np.asarray(batches_answers,np.int32)\n",
    "    \n",
    "    return batches_fact_stories,batches_questions,batches_answers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow placeholders\n",
    "\n",
    "tf_facts = tf.placeholder(tf.float32,[None,None,word_vec_dim])\n",
    "tf_questions = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "tf_answers = tf.placeholder(tf.int32,[None])\n",
    "training= tf.placeholder(tf.bool)\n",
    "\n",
    "#hyperparameters\n",
    "epochs = 256\n",
    "learning_rate = 0.001\n",
    "hidden_size = 100\n",
    "dropout_rate = 0.2\n",
    "beta = 0.0001 #l2 regularization scale\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpd = tf.transpose(tf.convert_to_tensor(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def layer_norm(inputs,scope,scale=True,layer_norm=True,epsilon = 1e-5):\n",
    "    \n",
    "    if layer_norm == True:\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            if scale == False:\n",
    "                scale = tf.ones([inputs.get_shape()[1]],tf.float32)\n",
    "            else:\n",
    "                scale = tf.get_variable(\"scale\", shape=[inputs.get_shape()[1]],\n",
    "                        initializer=tf.ones_initializer())\n",
    "        \n",
    "        \n",
    "        ## ignored shift - bias will be externally added which can produce shift\n",
    "\n",
    "        mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n",
    "        \n",
    "        LN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean))\n",
    "        \n",
    "        return LN\n",
    "    else:\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_GRU(inp,hidden,seq_len,scope):\n",
    "    \n",
    "    #inp shape = batch_size x seq_len x vector_dimension\n",
    "    inp = tf.transpose(inp,[1,0,2])\n",
    "    \n",
    "    #now inp shape = seq_len x batch_size x vector_dimension\n",
    "    \n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hiddenf = hidden\n",
    "    hiddenb = hidden\n",
    "\n",
    "    hidden_size = hidden.get_shape()[-1]\n",
    "    word_vec_dim = inp.get_shape()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[6,word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "        u = tf.get_variable(\"u\", shape=[6,hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        b = tf.get_variable(\"b\", shape=[6,hidden_size],initializer=tf.zeros_initializer())\n",
    "        \n",
    "    i = 0\n",
    "    j = seq_len - 1\n",
    "    \n",
    "    def cond(i,j,hiddenf,hiddenb,hidden_forward,hidden_backward):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hiddenf,hiddenb,hidden_forward,hidden_backward):\n",
    "        \n",
    "        xf = inp[i]\n",
    "        xb = inp[j]\n",
    "\n",
    "        # FORWARD GRU EQUATIONS:\n",
    "        z = tf.sigmoid(layer_norm( tf.matmul(xf,w[0]) + tf.matmul(hidden,u[0]) , scope+\"f_z\")+ b[0])\n",
    "        r = tf.sigmoid(layer_norm( tf.matmul(xf,w[1]) + tf.matmul(hidden,u[1]) , scope+\"f_r\")+ b[1])\n",
    "        h_ = tf.tanh(layer_norm( tf.matmul(xf,w[2]) + tf.multiply(r,tf.matmul(hiddenf,u[2])),scope+\"f_h\") + b[2])\n",
    "        hiddenf = tf.multiply(z,h_) + tf.multiply((1-z),hiddenf)\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,hiddenf)\n",
    "        \n",
    "        # BACKWARD GRU EQUATIONS:\n",
    "        z = tf.sigmoid(layer_norm( tf.matmul(xb,w[3]) + tf.matmul(hidden,u[3]) , scope+\"b_z\")+ b[0])\n",
    "        r = tf.sigmoid(layer_norm( tf.matmul(xb,w[4]) + tf.matmul(hidden,u[4]) , scope+\"b_r\")+ b[1])\n",
    "        h_ = tf.tanh(layer_norm( tf.matmul(xb,w[5]) + tf.multiply(r,tf.matmul(hiddenb,u[5])),scope+\"b_h\") + b[2])\n",
    "        hiddenb = tf.multiply(z,h_) + tf.multiply((1-z),hiddenb)\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(j,hiddenb)\n",
    "        \n",
    "        \n",
    "        return i+1,j-1,hiddenf,hiddenb,hidden_forward,hidden_backward\n",
    "    \n",
    "    _,_,_,_,hidden_forward,hidden_backward = tf.while_loop(cond,body,[i,j,\n",
    "                                                                        hiddenf,\n",
    "                                                                        hiddenb,\n",
    "                                                                        hidden_forward,\n",
    "                                                                        hidden_backward])\n",
    "    \n",
    "    forward = hidden_forward.stack()\n",
    "    backward = hidden_backward.stack()\n",
    "    \n",
    "    hidden_list = (forward + backward)\n",
    "    \n",
    "    #forward\\backward\\hidden_list shape = seq_len x  batch_size x hidden_size\n",
    "    \n",
    "    hidden_list = tf.transpose(hidden_list,[1,0,2])\n",
    "    \n",
    "    return hidden_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(inp,hidden,\n",
    "        seq_len,scope):\n",
    "    \n",
    "    #inp shape = batch_size x seq_len x vector_dimension\n",
    "    inp = tf.transpose(inp,[1,0,2])\n",
    "\n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_size = hidden.get_shape()[-1]\n",
    "    word_vec_dim = inp.get_shape()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[3,word_vec_dim, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "        u = tf.get_variable(\"u\", shape=[3,hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        b = tf.get_variable(\"b\", shape=[3,hidden_size],initializer=tf.zeros_initializer())\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def cond(i,hidden,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_lists):\n",
    "        \n",
    "        x = inp[i]\n",
    " \n",
    "        # GRU EQUATIONS:\n",
    "        z = tf.sigmoid(layer_norm( tf.matmul(x,w[0]) + tf.matmul(hidden,u[0]) , scope+\"_z\")+ b[0])\n",
    "        r = tf.sigmoid(layer_norm( tf.matmul(x,w[1]) + tf.matmul(hidden,u[1]) , scope+\"_r\")+ b[1])\n",
    "        h_ = tf.tanh(layer_norm( tf.matmul(x,w[2]) + tf.multiply(r,tf.matmul(hidden,u[2])),scope+\"_h\") + b[2])\n",
    "        hidden = tf.multiply(z,h_) + tf.multiply((1-z),hidden)\n",
    "\n",
    "        hidden_lists = hidden_lists.write(i,hidden)\n",
    "        \n",
    "        return i+1,hidden,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hidden,hidden_lists])\n",
    "    \n",
    "    hidden_lists = hidden_lists.stack()\n",
    "    hidden_lists = tf.transpose(hidden_lists,[1,0,2])\n",
    "    \n",
    "    return hidden_lists\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_attention_based_GRU(qr,facts,scope):\n",
    "    \n",
    "    qr_vec_size = qr.get_shape()[-1]\n",
    "    p_vec_size = facts.get_shape()[-1]\n",
    "    hidden_size = qr_vec_size\n",
    "        \n",
    "    seq_len = tf.shape(facts)[1]\n",
    "    question_len = tf.shape(qr)[1]\n",
    "    batch_size = tf.shape(facts)[0]\n",
    "    \n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    vp = tf.zeros([batch_size,qr_vec_size+p_vec_size],tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        wqu = tf.get_variable(\"wqu\", shape=[qr_vec_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "        wpu = tf.get_variable(\"wpu\", shape=[p_vec_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        wvp = tf.get_variable(\"wvp\", shape=[qr_vec_size+p_vec_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        v = tf.get_variable(\"v\", shape=[hidden_size, 1],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        wg = tf.get_variable(\"wg\", shape=[qr_vec_size+p_vec_size, 1],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        \n",
    "    # facts shape = batch_size x facts_num x p_vec_size\n",
    "\n",
    "    facts = tf.transpose(facts,[1,0,2])\n",
    "    \n",
    "    # now facts shape = facts_num x batch_size x p_vec_size\n",
    "    \n",
    "    # qr shape = batch_size x question_len x qr_vec_size\n",
    "    qr = tf.transpose(qr,[1,0,2])\n",
    "    # now qr shape =  question_len x batch_size x qr_vec_size\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    def cond(i,vp,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,vp,hidden_lists):\n",
    "        \n",
    "        qr_part = tf.reshape(qr,[question_len*batch_size,qr_vec_size])\n",
    "        qr_part = tf.matmul(qr_part,wqu)\n",
    "        qr_part = tf.reshape(qr_part,[question_len,batch_size,hidden_size])\n",
    "\n",
    "        facts_part = tf.matmul(facts[i],wpu)\n",
    "        facts_part = tf.reshape(facts_part,[1,batch_size,hidden_size])\n",
    "        \n",
    "        vp_part = tf.matmul(vp,wvp)\n",
    "        vp_part = tf.reshape(vp_part,[1,batch_size,hidden_size])\n",
    "        \n",
    "    \n",
    "        s = tf.tanh(qr_part + facts_part + vp_part)\n",
    "        s = tf.reshape(s,[question_len*batch_size,hidden_size])\n",
    "        \n",
    "        s = tf.matmul(s,v)\n",
    "        \n",
    "        s = tf.reshape(s,[question_len,batch_size])\n",
    "        s = tf.transpose(s,[1,0])\n",
    "        \n",
    "        # now s shape = batch_size x question_len\n",
    "        \n",
    "        a = tf.nn.softmax(s)\n",
    "        \n",
    "        # a shape = batch_size x question_len\n",
    "        \n",
    "        a = tf.transpose(a)\n",
    "        \n",
    "        # a shape = question_len x batch_size\n",
    "        \n",
    "        a = tf.reshape(a,[question_len,batch_size,1])\n",
    "        \n",
    "        c = tf.multiply(a,qr)\n",
    "        \n",
    "        c = tf.reduce_sum(c,0)\n",
    "        \n",
    "        # c (context vector) shape = batch_size x qr_vec_size\n",
    "     \n",
    "        uc = tf.concat([facts[i],c],1)\n",
    "        g = tf.nn.sigmoid(tf.matmul(uc,wg))\n",
    "        uc_ = tf.multiply(g,uc)\n",
    "        \n",
    "        # uc_ shape =batch_size x (qr_vec_size+p_vec_size)\n",
    "        \n",
    "        vp = bi_GRU(tf.reshape(vp,[batch_size,1,qr_vec_size+p_vec_size]),uc_,1,scope)\n",
    "        \n",
    "        vp = tf.reshape(vp,[batch_size,qr_vec_size+p_vec_size])\n",
    "        \n",
    "        hidden_lists = hidden_lists.write(i,vp)\n",
    "   \n",
    "        return i+1,vp,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,vp,hidden_lists])\n",
    "    \n",
    "    hidden_lists = hidden_lists.stack()\n",
    "    \n",
    "    # hidden_lists shape = facts_num x batch_size x (p_vec_dim + qr_vec_dim)\n",
    "    \n",
    "    hidden_lists = tf.transpose(hidden_lists,[1,0,2])\n",
    "    \n",
    "    # now hidden_lists shape = batch_size x facts_num x (p_vec_dim + qr_vec_dim)\n",
    "    \n",
    "    return hidden_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_matching_attention(vp,scope):\n",
    "    \n",
    "    hidden_size = vp.get_shape()[-1]\n",
    "    seq_len = tf.shape(vp)[1]\n",
    "    batch_size = tf.shape(vp)[0]\n",
    "    \n",
    "    hidden_lists = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    hp = tf.zeros([batch_size,hidden_size+hidden_size],tf.float32)\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        wvp_whole = tf.get_variable(\"wvp1\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer= regularizer)\n",
    "        wvp_single = tf.get_variable(\"wvp2\", shape=[hidden_size, hidden_size],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        v = tf.get_variable(\"v\", shape=[hidden_size, 1],\n",
    "                      initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                      regularizer=regularizer)\n",
    "        \n",
    "    # vp shape = batch_size x facts_num x hidden_size\n",
    "\n",
    "    vp = tf.transpose(vp,[1,0,2])\n",
    "    \n",
    "    # now vp shape = facts_num x batch_size x hidden_size\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    def cond(i,hp,hidden_lists):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,hp,hidden_lists):\n",
    "        \n",
    "        vp_whole_part = tf.reshape(vp,[seq_len*batch_size,hidden_size])\n",
    "        vp_whole_part = tf.matmul(vp_whole_part,wvp_whole)\n",
    "        vp_whole_part = tf.reshape(vp_whole_part,[seq_len,batch_size,hidden_size])\n",
    "        \n",
    "        vp_single_part = tf.matmul(vp[i],wvp_single)\n",
    "        vp_single_part = tf.reshape(vp_single_part,[1,batch_size,hidden_size])\n",
    "    \n",
    "        s = tf.tanh(vp_whole_part + vp_single_part)\n",
    "        s = tf.reshape(s,[seq_len*batch_size,hidden_size])\n",
    "        \n",
    "        s = tf.matmul(s,v)\n",
    "        \n",
    "        s = tf.reshape(s,[seq_len,batch_size])\n",
    "        s = tf.transpose(s,[1,0])\n",
    "        \n",
    "        # now s shape = batch_size x facts_num\n",
    "        \n",
    "        a = tf.nn.softmax(s)\n",
    "        \n",
    "        # a shape = batch_size x facts_num\n",
    "        \n",
    "        a = tf.transpose(a)\n",
    "        \n",
    "        # a shape = facts_num x batch_size\n",
    "        \n",
    "        a = tf.reshape(a,[seq_len,batch_size,1])\n",
    "        \n",
    "        c = tf.multiply(a,vp)\n",
    "        \n",
    "        c = tf.reduce_sum(c,0)\n",
    "        \n",
    "        # c (context vector) shape = batch_size x hidden_size\n",
    "     \n",
    "        vpc = tf.concat([vp[i],c],1)\n",
    "        \n",
    "        # vpc shape =batch_size x 2*hidden_size\n",
    "        \n",
    "        hp = bi_GRU(tf.reshape(hp,[batch_size,1,hidden_size+hidden_size]),vpc,1,scope)\n",
    "        \n",
    "        hp = tf.reshape(hp,[batch_size,hidden_size+hidden_size])\n",
    "        \n",
    "        hidden_lists = hidden_lists.write(i,hp)\n",
    "   \n",
    "        return i+1,hp,hidden_lists\n",
    "    \n",
    "    _,_,hidden_lists = tf.while_loop(cond,body,[i,hp,hidden_lists])\n",
    "    \n",
    "    hidden_lists = hidden_lists.stack()\n",
    "    \n",
    "    # hidden_lists shape = facts_num x batch_size x hidden_size\n",
    "    \n",
    "    hidden_lists = tf.transpose(hidden_lists,[1,0,2])\n",
    "    \n",
    "    # now hidden_lists shape = batch_size x facts_num x hidden_size\n",
    "    \n",
    "    return hidden_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_NET(tf_facts,tf_questions):\n",
    "    \n",
    "    tf_batch_size = tf.shape(tf_facts)[0]\n",
    "    facts_num = tf.shape(tf_facts)[1]\n",
    "    \n",
    "    question_len = tf.shape(tf_questions)[1]\n",
    "    \n",
    "    hidden = tf.zeros([tf_batch_size,hidden_size],tf.float32)\n",
    "    \n",
    "    # word level question encoding\n",
    "    qr = bi_GRU(tf_questions,hidden,question_len,scope=\"question_encoding\")\n",
    "    \n",
    "    #now qr shape = batch_size x question_len x 2*hidden_size\n",
    "    \n",
    "    \n",
    "    # Encoding facts\n",
    "    tf_facts = bi_GRU(tf_facts,hidden,\n",
    "                      facts_num,\n",
    "                      scope=\"facts_encoding\")\n",
    "    \n",
    "    # now tf_facts shape = batch_size x facts_num x 2*hidden_size\n",
    "    \n",
    "    tf_facts = tf.layers.dropout(tf_facts,dropout_rate,training=training)\n",
    "    \n",
    "    # Producing question aware document representation through gated attention based RNN. \n",
    "    \n",
    "    qr_facts = gated_attention_based_GRU(qr,tf_facts,\"gated_attention\")\n",
    "    \n",
    "    qr_facts = tf.layers.dropout(qr_facts,dropout_rate,training=training)\n",
    "    \n",
    "    # Producing final passage representation through self-matching attention.\n",
    "    \n",
    "    hp_list = self_matching_attention(qr_facts,\"self_matching_attention\")\n",
    "    \n",
    "    hp_list = tf.layers.dropout(hp_list,dropout_rate,training=training)\n",
    "    \n",
    "    predicted_word = GRU(hp_list,hidden,facts_num,\"answer_layer\")\n",
    "    predicted_word = predicted_word[:,facts_num-1]\n",
    "    \n",
    "    predicted_word = tf.layers.dropout(predicted_word,dropout_rate,training=training)\n",
    "    \n",
    "    # Convert to pre-softmax probability distribution\n",
    "    y = tf.matmul(predicted_word,wpd)\n",
    "    \n",
    "    return y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = R_NET(tf_facts,tf_questions)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "#reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "regularization = 0#tf.contrib.layers.apply_regularization(regularizer, tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, \n",
    "                                                                     labels=tf_answers))+regularization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "model_output = tf.nn.softmax(model_output)\n",
    "\n",
    "#Evaluate model\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(model_output,1),tf.int32),tf_answers)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss= 16.367, Accuracy= 0.000\n",
      "Iter 20, Loss= 1.889, Accuracy= 21.875\n",
      "Iter 40, Loss= 1.664, Accuracy= 28.125\n",
      "Iter 60, Loss= 1.669, Accuracy= 28.125\n",
      "\n",
      "Epoch 1, Validation Loss= 1.308, validation Accuracy= 38.400%\n",
      "Epoch 1, Average Training Loss= 1.997, Average Training Accuracy= 26.016%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.674, Accuracy= 25.000\n",
      "Iter 20, Loss= 1.476, Accuracy= 41.406\n",
      "Iter 40, Loss= 1.413, Accuracy= 36.719\n",
      "Iter 60, Loss= 1.223, Accuracy= 52.344\n",
      "\n",
      "Epoch 2, Validation Loss= 1.089, validation Accuracy= 48.000%\n",
      "Epoch 2, Average Training Loss= 1.413, Average Training Accuracy= 41.004%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.418, Accuracy= 40.625\n",
      "Iter 20, Loss= 1.292, Accuracy= 46.875\n",
      "Iter 40, Loss= 1.256, Accuracy= 53.125\n",
      "Iter 60, Loss= 1.409, Accuracy= 40.625\n",
      "\n",
      "Epoch 3, Validation Loss= 1.080, validation Accuracy= 48.200%\n",
      "Epoch 3, Average Training Loss= 1.293, Average Training Accuracy= 45.826%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.245, Accuracy= 46.875\n",
      "Iter 20, Loss= 1.229, Accuracy= 50.781\n",
      "Iter 40, Loss= 1.201, Accuracy= 42.969\n",
      "Iter 60, Loss= 1.231, Accuracy= 45.312\n",
      "\n",
      "Epoch 4, Validation Loss= 1.013, validation Accuracy= 48.500%\n",
      "Epoch 4, Average Training Loss= 1.214, Average Training Accuracy= 46.719%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.152, Accuracy= 49.219\n",
      "Iter 20, Loss= 1.117, Accuracy= 50.000\n",
      "Iter 40, Loss= 1.085, Accuracy= 46.875\n",
      "Iter 60, Loss= 1.224, Accuracy= 43.750\n",
      "\n",
      "Epoch 5, Validation Loss= 0.984, validation Accuracy= 46.700%\n",
      "Epoch 5, Average Training Loss= 1.173, Average Training Accuracy= 46.116%\n",
      "\n",
      "Iter 0, Loss= 1.116, Accuracy= 43.750\n",
      "Iter 20, Loss= 1.081, Accuracy= 49.219\n",
      "Iter 40, Loss= 1.142, Accuracy= 42.969\n",
      "Iter 60, Loss= 1.127, Accuracy= 44.531\n",
      "\n",
      "Epoch 6, Validation Loss= 0.919, validation Accuracy= 48.700%\n",
      "Epoch 6, Average Training Loss= 1.110, Average Training Accuracy= 45.949%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.111, Accuracy= 47.656\n",
      "Iter 20, Loss= 1.155, Accuracy= 38.281\n",
      "Iter 40, Loss= 1.158, Accuracy= 41.406\n",
      "Iter 60, Loss= 0.955, Accuracy= 46.875\n",
      "\n",
      "Epoch 7, Validation Loss= 0.952, validation Accuracy= 48.200%\n",
      "Epoch 7, Average Training Loss= 1.092, Average Training Accuracy= 46.105%\n",
      "\n",
      "Iter 0, Loss= 1.137, Accuracy= 39.844\n",
      "Iter 20, Loss= 1.014, Accuracy= 45.312\n",
      "Iter 40, Loss= 1.071, Accuracy= 39.844\n",
      "Iter 60, Loss= 1.077, Accuracy= 43.750\n",
      "\n",
      "Epoch 8, Validation Loss= 0.945, validation Accuracy= 46.700%\n",
      "Epoch 8, Average Training Loss= 1.068, Average Training Accuracy= 46.596%\n",
      "\n",
      "Iter 0, Loss= 1.121, Accuracy= 43.750\n",
      "Iter 20, Loss= 1.041, Accuracy= 44.531\n",
      "Iter 40, Loss= 1.006, Accuracy= 49.219\n",
      "Iter 60, Loss= 1.070, Accuracy= 46.875\n",
      "\n",
      "Epoch 9, Validation Loss= 0.911, validation Accuracy= 48.900%\n",
      "Epoch 9, Average Training Loss= 1.064, Average Training Accuracy= 46.507%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 1.068, Accuracy= 44.531\n",
      "Iter 20, Loss= 1.020, Accuracy= 47.656\n",
      "Iter 40, Loss= 1.056, Accuracy= 48.438\n",
      "Iter 60, Loss= 0.971, Accuracy= 48.438\n",
      "\n",
      "Epoch 10, Validation Loss= 0.938, validation Accuracy= 47.500%\n",
      "Epoch 10, Average Training Loss= 1.042, Average Training Accuracy= 47.299%\n",
      "\n",
      "Iter 0, Loss= 1.018, Accuracy= 50.000\n",
      "Iter 20, Loss= 1.074, Accuracy= 46.094\n",
      "Iter 40, Loss= 1.039, Accuracy= 46.875\n",
      "Iter 60, Loss= 1.111, Accuracy= 41.406\n",
      "\n",
      "Epoch 11, Validation Loss= 0.905, validation Accuracy= 48.300%\n",
      "Epoch 11, Average Training Loss= 1.037, Average Training Accuracy= 47.321%\n",
      "\n",
      "Iter 0, Loss= 1.065, Accuracy= 44.531\n",
      "Iter 20, Loss= 1.094, Accuracy= 38.281\n",
      "Iter 40, Loss= 0.978, Accuracy= 54.688\n",
      "Iter 60, Loss= 1.037, Accuracy= 44.531\n",
      "\n",
      "Epoch 12, Validation Loss= 0.923, validation Accuracy= 46.800%\n",
      "Epoch 12, Average Training Loss= 1.049, Average Training Accuracy= 47.054%\n",
      "\n",
      "Iter 0, Loss= 0.951, Accuracy= 51.562\n",
      "Iter 20, Loss= 0.987, Accuracy= 51.562\n",
      "Iter 40, Loss= 1.107, Accuracy= 45.312\n",
      "Iter 60, Loss= 1.074, Accuracy= 44.531\n",
      "\n",
      "Epoch 13, Validation Loss= 0.912, validation Accuracy= 47.700%\n",
      "Epoch 13, Average Training Loss= 1.031, Average Training Accuracy= 48.013%\n",
      "\n",
      "Iter 0, Loss= 1.009, Accuracy= 49.219\n",
      "Iter 20, Loss= 1.076, Accuracy= 45.312\n",
      "Iter 40, Loss= 0.943, Accuracy= 48.438\n",
      "Iter 60, Loss= 1.030, Accuracy= 54.688\n",
      "\n",
      "Epoch 14, Validation Loss= 0.929, validation Accuracy= 48.700%\n",
      "Epoch 14, Average Training Loss= 1.026, Average Training Accuracy= 47.902%\n",
      "\n",
      "Iter 0, Loss= 1.008, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.966, Accuracy= 50.000\n",
      "Iter 40, Loss= 1.032, Accuracy= 41.406\n",
      "Iter 60, Loss= 0.935, Accuracy= 50.781\n",
      "\n",
      "Epoch 15, Validation Loss= 0.904, validation Accuracy= 48.700%\n",
      "Epoch 15, Average Training Loss= 1.029, Average Training Accuracy= 47.188%\n",
      "\n",
      "Iter 0, Loss= 0.908, Accuracy= 52.344\n",
      "Iter 20, Loss= 1.083, Accuracy= 44.531\n",
      "Iter 40, Loss= 0.987, Accuracy= 53.125\n",
      "Iter 60, Loss= 1.026, Accuracy= 45.312\n",
      "\n",
      "Epoch 16, Validation Loss= 0.919, validation Accuracy= 49.500%\n",
      "Epoch 16, Average Training Loss= 1.029, Average Training Accuracy= 47.679%\n",
      "Checkpoint created!\n",
      "\n",
      "Iter 0, Loss= 0.977, Accuracy= 53.125\n",
      "Iter 20, Loss= 1.002, Accuracy= 48.438\n",
      "Iter 40, Loss= 0.983, Accuracy= 46.094\n",
      "Iter 60, Loss= 0.956, Accuracy= 50.781\n",
      "\n",
      "Epoch 17, Validation Loss= 0.924, validation Accuracy= 48.900%\n",
      "Epoch 17, Average Training Loss= 1.003, Average Training Accuracy= 49.364%\n",
      "\n",
      "Iter 0, Loss= 1.041, Accuracy= 49.219\n",
      "Iter 20, Loss= 0.987, Accuracy= 53.125\n",
      "Iter 40, Loss= 0.987, Accuracy= 52.344\n",
      "Iter 60, Loss= 1.073, Accuracy= 45.312\n",
      "\n",
      "Epoch 18, Validation Loss= 0.950, validation Accuracy= 47.400%\n",
      "Epoch 18, Average Training Loss= 1.004, Average Training Accuracy= 49.442%\n",
      "\n",
      "Iter 0, Loss= 1.057, Accuracy= 52.344\n",
      "Iter 20, Loss= 1.086, Accuracy= 46.875\n",
      "Iter 40, Loss= 0.940, Accuracy= 53.125\n",
      "Iter 60, Loss= 0.963, Accuracy= 54.688\n",
      "\n",
      "Epoch 19, Validation Loss= 0.932, validation Accuracy= 46.700%\n",
      "Epoch 19, Average Training Loss= 1.001, Average Training Accuracy= 50.022%\n",
      "\n",
      "Iter 0, Loss= 0.911, Accuracy= 57.812\n",
      "Iter 20, Loss= 0.896, Accuracy= 60.156\n",
      "Iter 40, Loss= 1.105, Accuracy= 47.656\n",
      "Iter 60, Loss= 0.979, Accuracy= 44.531\n",
      "\n",
      "Epoch 20, Validation Loss= 0.929, validation Accuracy= 47.000%\n",
      "Epoch 20, Average Training Loss= 0.984, Average Training Accuracy= 50.759%\n",
      "\n",
      "Iter 0, Loss= 0.881, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.944, Accuracy= 48.438\n",
      "Iter 40, Loss= 1.066, Accuracy= 46.094\n",
      "Iter 60, Loss= 0.977, Accuracy= 52.344\n",
      "\n",
      "Epoch 21, Validation Loss= 0.966, validation Accuracy= 47.700%\n",
      "Epoch 21, Average Training Loss= 0.989, Average Training Accuracy= 50.614%\n",
      "\n",
      "Iter 0, Loss= 1.103, Accuracy= 42.188\n",
      "Iter 20, Loss= 0.908, Accuracy= 55.469\n",
      "Iter 40, Loss= 0.913, Accuracy= 60.156\n",
      "Iter 60, Loss= 0.912, Accuracy= 52.344\n",
      "\n",
      "Epoch 22, Validation Loss= 0.952, validation Accuracy= 48.400%\n",
      "Epoch 22, Average Training Loss= 0.985, Average Training Accuracy= 50.781%\n",
      "\n",
      "Iter 0, Loss= 0.913, Accuracy= 52.344\n",
      "Iter 20, Loss= 0.916, Accuracy= 56.250\n",
      "Iter 40, Loss= 1.001, Accuracy= 57.031\n",
      "Iter 60, Loss= 0.923, Accuracy= 54.688\n",
      "\n",
      "Epoch 23, Validation Loss= 0.966, validation Accuracy= 47.200%\n",
      "Epoch 23, Average Training Loss= 0.980, Average Training Accuracy= 51.719%\n",
      "\n",
      "Iter 0, Loss= 1.048, Accuracy= 46.875\n",
      "Iter 20, Loss= 1.000, Accuracy= 51.562\n",
      "Iter 40, Loss= 0.839, Accuracy= 62.500\n",
      "Iter 60, Loss= 1.020, Accuracy= 42.188\n",
      "\n",
      "Epoch 24, Validation Loss= 0.975, validation Accuracy= 45.900%\n",
      "Epoch 24, Average Training Loss= 0.962, Average Training Accuracy= 52.210%\n",
      "\n",
      "Iter 0, Loss= 0.924, Accuracy= 50.000\n",
      "Iter 20, Loss= 0.977, Accuracy= 45.312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3488cd52f0d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                   \u001b[0mtf_questions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatches_train_questions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                   \u001b[0mtf_answers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatches_train_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                                                   training: True})\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 1   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    best_val_loss=2**30\n",
    "    prev_val_acc=0\n",
    "    patience = 20\n",
    "    impatience = 0\n",
    "    display_step = 20\n",
    "    min_epoch = 20\n",
    "            \n",
    "    batch_size = 128\n",
    "    \n",
    "    while step <= epochs:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        batches_train_fact_stories,batches_train_questions,batches_train_answers = create_batches(train_fact_stories,train_questions,train_answers,batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_train_questions)):\n",
    "            \n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,acc = sess.run([optimizer,cost,accuracy],\n",
    "                                       feed_dict={tf_facts: batches_train_fact_stories[i], \n",
    "                                                  tf_questions: batches_train_questions[i], \n",
    "                                                  tf_answers: batches_train_answers[i],\n",
    "                                                  training: True})\n",
    "\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "                \n",
    "            if i%display_step == 0:\n",
    "                print \"Iter \"+str(i)+\", Loss= \"+\\\n",
    "                      \"{:.3f}\".format(loss)+\", Accuracy= \"+\\\n",
    "                      \"{:.3f}\".format(acc*100)\n",
    "                        \n",
    "        avg_loss = total_loss/len(batches_train_questions) \n",
    "        avg_acc = total_acc/len(batches_train_questions)  \n",
    "        \n",
    "        loss_list.append(avg_loss) \n",
    "        acc_list.append(avg_acc) \n",
    "\n",
    "        val_batch_size = 100 #(should be able to divide total no. of validation samples without remainder)\n",
    "        batches_val_fact_stories,batches_val_questions,batches_val_answers = create_batches(val_fact_stories,val_questions,val_answers,val_batch_size)\n",
    "        \n",
    "        for i in xrange(len(batches_val_questions)):\n",
    "            val_loss, val_acc = sess.run([cost, accuracy], \n",
    "                                         feed_dict={tf_facts: batches_val_fact_stories[i], \n",
    "                                                    tf_questions: batches_val_questions[i], \n",
    "                                                    tf_answers: batches_val_answers[i],\n",
    "                                                    training: False})\n",
    "            total_val_loss += val_loss\n",
    "            total_val_acc += val_acc\n",
    "                      \n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(batches_val_questions) \n",
    "        avg_val_acc = total_val_acc/len(batches_val_questions) \n",
    "             \n",
    "        val_loss_list.append(avg_val_loss) \n",
    "        val_acc_list.append(avg_val_acc) \n",
    "    \n",
    "\n",
    "        print \"\\nEpoch \" + str(step) + \", Validation Loss= \" + \\\n",
    "                \"{:.3f}\".format(avg_val_loss) + \", validation Accuracy= \" + \\\n",
    "                \"{:.3f}%\".format(avg_val_acc*100)+\"\"\n",
    "        print \"Epoch \" + str(step) + \", Average Training Loss= \" + \\\n",
    "              \"{:.3f}\".format(avg_loss) + \", Average Training Accuracy= \" + \\\n",
    "              \"{:.3f}%\".format(avg_acc*100)+\"\"\n",
    "        \n",
    "        impatience += 1\n",
    "        \n",
    "        if avg_val_acc >= best_val_acc:\n",
    "            best_val_acc = avg_val_acc\n",
    "            saver.save(sess, 'DMN_Model_Backup/model.ckpt') \n",
    "            print \"Checkpoint created!\"\n",
    "\n",
    "\n",
    "    \n",
    "        if avg_val_loss <= best_val_loss: \n",
    "            impatience=0\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "\n",
    "        \n",
    "        if impatience > patience and step>min_epoch:\n",
    "            print \"\\nEarly Stopping since best validation loss not decreasing for \"+str(patience)+\" epochs.\"\n",
    "            break\n",
    "            \n",
    "        print \"\"\n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    print \"\\nOptimization Finished!\\n\"\n",
    "    \n",
    "    print \"Best Validation Accuracy: %.3f%%\"%((best_val_acc*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving logs about change of training and validation loss and accuracy over epochs in another file.\n",
    "\n",
    "import h5py\n",
    "\n",
    "file = h5py.File('Training_logs_DMN_plus.h5','w')\n",
    "file.create_dataset('val_acc', data=np.array(val_acc_list))\n",
    "file.create_dataset('val_loss', data=np.array(val_loss_list))\n",
    "file.create_dataset('acc', data=np.array(acc_list))\n",
    "file.create_dataset('loss', data=np.array(loss_list))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEPCAYAAACukxSbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGBNFLOgbY0wU+f/s//tIJhXT5QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f139fa9b590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEPCAYAAAC5sYRSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUnAXEUlA/x8xQlLosRFhYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f139f5b1c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "log = h5py.File('Training_logs_DMN_plus.h5','r+') # Loading logs about change of training and validation loss and accuracy over epochs\n",
    "\n",
    "y1 = log['val_acc'][...]\n",
    "y2 = log['acc'][...]\n",
    "\n",
    "x = np.arange(1,len(y1)+1,1) # (1 = starting epoch, len(y1) = no. of epochs, 1 = step) \n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Accuracy') \n",
    "plt.plot(x,y2,'r',label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "y1 = log['val_loss'][...]\n",
    "y2 = log['loss'][...]\n",
    "\n",
    "plt.plot(x,y1,'b',label='Validation Loss')\n",
    "plt.plot(x,y2,'r',label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for the model...\n",
      "INFO:tensorflow:Restoring parameters from DMN_Model_Backup/model.ckpt\n",
      "\n",
      "RESTORATION COMPLETE\n",
      "\n",
      "Testing Model Performance...\n",
      "\n",
      "Test Loss= 0.914, Test Accuracy= 50.100%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: # Begin session\n",
    "    \n",
    "    print 'Loading pre-trained weights for the model...'\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, 'DMN_Model_Backup/model.ckpt')\n",
    "    sess.run(tf.global_variables())\n",
    "    print '\\nRESTORATION COMPLETE\\n'\n",
    "    \n",
    "    print 'Testing Model Performance...'\n",
    "    \n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "    \n",
    "    test_batch_size = 100 #(should be able to divide total no. of test samples without remainder)\n",
    "    batches_test_fact_stories,batches_test_questions,batches_test_answers = create_batches(test_fact_stories,test_questions,test_answers,test_batch_size)\n",
    "        \n",
    "    for i in xrange(len(batches_test_questions)):\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], \n",
    "                                        feed_dict={tf_facts: batches_test_fact_stories[i], \n",
    "                                                   tf_questions: batches_test_questions[i], \n",
    "                                                   tf_answers: batches_test_answers[i],\n",
    "                                                   training: False})\n",
    "        total_test_loss += test_loss\n",
    "        total_test_acc += test_acc\n",
    "                      \n",
    "            \n",
    "    avg_test_loss = total_test_loss/len(batches_test_questions) \n",
    "    avg_test_acc = total_test_acc/len(batches_test_questions) \n",
    "\n",
    "\n",
    "    print \"\\nTest Loss= \" + \\\n",
    "          \"{:.3f}\".format(avg_test_loss) + \", Test Accuracy= \" + \\\n",
    "          \"{:.3f}%\".format(avg_test_acc*100)+\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
